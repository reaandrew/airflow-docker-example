name: airflow3-bitnami

x-airflow-common: &airflow-common
  image: ${AIRFLOW_IMAGE_NAME}
  env_file:
    - .env
  environment:
    # Core / DB
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:${POSTGRES_PASSWORD}@postgres:5432/airflow

    # Celery via Redis (no RabbitMQ needed)
    AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD}@redis:6379/0
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:${POSTGRES_PASSWORD}@postgres:5432/airflow

    # Secrets
    AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}
    AIRFLOW__API__SECRET_KEY: ${API_SECRET_KEY}
    AIRFLOW__WEBSERVER__SECRET_KEY: ${WEBSERVER_SECRET_KEY}

    # Behaviour / auth
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "True"
    AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "False"
    
    # Simple Auth Manager
    AIRFLOW__CORE__AUTH_MANAGER: airflow.api_fastapi.auth.managers.simple.simple_auth_manager.SimpleAuthManager
    AIRFLOW__SIMPLE_AUTH_MANAGER__USERS: "admin:admin,airflow:admin"

    # Admin user (official Airflow image)
    _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_USERNAME}
    _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_PASSWORD}
    _AIRFLOW_WWW_USER_EMAIL: ${AIRFLOW_ADMIN_EMAIL}
    _AIRFLOW_WWW_USER_FIRSTNAME: ${AIRFLOW_FIRSTNAME}
    _AIRFLOW_WWW_USER_LASTNAME: ${AIRFLOW_LASTNAME}

  user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-0}"
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
  restart: unless-stopped

services:
  postgres:
    image: ${POSTGRES_IMAGE_NAME}
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
    volumes:
      - postgres-data:/var/lib/postgresql/data
    restart: unless-stopped

  redis:
    image: ${REDIS_IMAGE_NAME}
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      ALLOW_EMPTY_PASSWORD: "no"
    command: ["redis-server", "--appendonly", "yes", "--requirepass", "${REDIS_PASSWORD}"]
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 10
    volumes:
      - redis-data:/data
    restart: unless-stopped

  # Do NOT override the entrypoint; just run our commands using absolute paths
  airflow-init:
    <<: *airflow-common
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    command: ["bash", "-c", "airflow db migrate && airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password SuperSecret_Admin_Pw || airflow users create --username airflow --firstname Airflow --lastname Admin --role Admin --email admin@example.com --password airflow || echo 'User creation may have failed, but continuing...'"]
    restart: "no"

  airflow-apiserver:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    command: ["api-server"]
    ports:
      - "${AIRFLOW_WEB_PORT:-8080}:8080"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/health || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 20

  airflow-dag-processor:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: ["airflow", "dag-processor"]
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'airflow dag-processor' >/dev/null || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 20

  airflow-scheduler:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: ["airflow", "scheduler"]
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname) || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 20

  airflow-triggerer:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: ["airflow", "triggerer"]
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type TriggererJob --hostname $(hostname) || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 20

  airflow-worker:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: ["airflow", "celery", "worker"]
    healthcheck:
      test: ["CMD-SHELL", "python - <<'PY'\nfrom celery import Celery\napp=Celery(broker='redis://:${REDIS_PASSWORD}@redis:6379/0')\napp.control.ping(timeout=1)\nprint('ok')\nPY"]
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: "2g"

  airflow-flower:
    <<: *airflow-common
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: ["airflow", "celery", "flower", "--port", "5555"]
    ports:
      - "${FLOWER_PORT:-5555}:5555"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:5555/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20

volumes:
  postgres-data:
  redis-data:

networks:
  default:
    name: airflow-net

